{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent postprocessing: v1\n",
    "This file takes as input a sequence of data from the CNN, and attempts to refine that into a more accurate output command.\n",
    "\n",
    "Some credit belongs to https://github.com/harvitronix/five-video-classification-methods/blob/master/models.py for providing inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create references to important directories we will use over and over\n",
    "import os, sys\n",
    "DATA_HOME_DIR = '/home/nathan/olin/spring2017/line-follower/line-follower/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import modules\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import bcolz\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.metrics import categorical_crossentropy, categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nathan/olin/spring2017/line-follower/line-follower/data\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "\n",
    "path = DATA_HOME_DIR\n",
    "train_path=path + '/qea-square_2'\n",
    "valid_path=path + '/qea-square_3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_LEN = 512 # The number of columns in the CSV\n",
    "WINDOW_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "def concatCmdVelFlip(array):\n",
    "    \"\"\" Concatentaes and returns Cmd Vel array \"\"\"\n",
    "    return np.concatenate((array, array*-1)) # multiply by negative 1 for opposite turn\n",
    "\n",
    "def windows(X, Y, seq_len=10):\n",
    "    assert len(X) == len(Y)\n",
    "    \n",
    "    result = []\n",
    "    for index in range(X.shape[0] - seq_len):\n",
    "        result.append([X[index:index+seq_len], Y[index+seq_len-1]])\n",
    "    result = np.array(result)\n",
    "    np.random.shuffle(result)\n",
    "    return np.array(list(result[:,0])), np.array(list(result[:,1]))\n",
    "\n",
    "def loadData(folder):\n",
    "    Y = np.genfromtxt(folder+'/cmd_vel.csv', delimiter=',')[:,1] # only use turning angle\n",
    "    Y = concatCmdVelFlip(Y)\n",
    "    \n",
    "#     X_all = np.genfromtxt(folder+'/X_train_preds.csv', delimiter=',')\n",
    "    X_all = load_array(folder+'/X_train_features.b')\n",
    "    X_all = np.reshape(X_all, (len(X_all), INPUT_LEN))\n",
    "    \n",
    "    X_windowed, Y_windowed = windows(X_all, Y, WINDOW_SIZE)\n",
    "    \n",
    "    return X_windowed, Y_windowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = loadData(train_path);\n",
    "X_valid, Y_valid = loadData(valid_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_shape = (WINDOW_SIZE, INPUT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                    (None, 2048)          20979712    lstm_input_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 2048)          0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           1049088     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             513         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 22,029,313\n",
      "Trainable params: 22,029,313\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Sequential([\n",
    "            LSTM(2048, return_sequences=False, input_shape=in_shape),\n",
    "            Dropout(0.5),\n",
    "#             Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(1)\n",
    "        ])\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 508 samples, validate on 180 samples\n",
      "Epoch 1/150\n",
      "508/508 [==============================] - 0s - loss: 0.0859 - val_loss: 0.1005\n",
      "Epoch 2/150\n",
      "508/508 [==============================] - 0s - loss: 0.0805 - val_loss: 0.0961\n",
      "Epoch 3/150\n",
      "508/508 [==============================] - 0s - loss: 0.0880 - val_loss: 0.0960\n",
      "Epoch 4/150\n",
      "508/508 [==============================] - 0s - loss: 0.0845 - val_loss: 0.1035\n",
      "Epoch 5/150\n",
      "508/508 [==============================] - 0s - loss: 0.0859 - val_loss: 0.1049\n",
      "Epoch 6/150\n",
      "508/508 [==============================] - 0s - loss: 0.0835 - val_loss: 0.1152\n",
      "Epoch 7/150\n",
      "508/508 [==============================] - 0s - loss: 0.0816 - val_loss: 0.1164\n",
      "Epoch 8/150\n",
      "508/508 [==============================] - 0s - loss: 0.0824 - val_loss: 0.0907\n",
      "Epoch 9/150\n",
      "508/508 [==============================] - 0s - loss: 0.0886 - val_loss: 0.0991\n",
      "Epoch 10/150\n",
      "508/508 [==============================] - 0s - loss: 0.0849 - val_loss: 0.1009\n",
      "Epoch 11/150\n",
      "508/508 [==============================] - 0s - loss: 0.0833 - val_loss: 0.1205\n",
      "Epoch 12/150\n",
      "508/508 [==============================] - 0s - loss: 0.0865 - val_loss: 0.1221\n",
      "Epoch 13/150\n",
      "508/508 [==============================] - 0s - loss: 0.0885 - val_loss: 0.0981\n",
      "Epoch 14/150\n",
      "508/508 [==============================] - 0s - loss: 0.0771 - val_loss: 0.1065\n",
      "Epoch 15/150\n",
      "508/508 [==============================] - 0s - loss: 0.0764 - val_loss: 0.1286\n",
      "Epoch 16/150\n",
      "508/508 [==============================] - 0s - loss: 0.0787 - val_loss: 0.1016\n",
      "Epoch 17/150\n",
      "508/508 [==============================] - 0s - loss: 0.0762 - val_loss: 0.1005\n",
      "Epoch 18/150\n",
      "508/508 [==============================] - 0s - loss: 0.0781 - val_loss: 0.1056\n",
      "Epoch 19/150\n",
      "508/508 [==============================] - 0s - loss: 0.0753 - val_loss: 0.1027\n",
      "Epoch 20/150\n",
      "508/508 [==============================] - 0s - loss: 0.0737 - val_loss: 0.1000\n",
      "Epoch 21/150\n",
      "508/508 [==============================] - 0s - loss: 0.0728 - val_loss: 0.1052\n",
      "Epoch 22/150\n",
      "508/508 [==============================] - 0s - loss: 0.0801 - val_loss: 0.1091\n",
      "Epoch 23/150\n",
      "508/508 [==============================] - 0s - loss: 0.0788 - val_loss: 0.1207\n",
      "Epoch 24/150\n",
      "508/508 [==============================] - 0s - loss: 0.0762 - val_loss: 0.0979\n",
      "Epoch 25/150\n",
      "508/508 [==============================] - 0s - loss: 0.0777 - val_loss: 0.1024\n",
      "Epoch 26/150\n",
      "508/508 [==============================] - 0s - loss: 0.0770 - val_loss: 0.0976\n",
      "Epoch 27/150\n",
      "508/508 [==============================] - 0s - loss: 0.0716 - val_loss: 0.1020\n",
      "Epoch 28/150\n",
      "508/508 [==============================] - 0s - loss: 0.0719 - val_loss: 0.1204\n",
      "Epoch 29/150\n",
      "508/508 [==============================] - 0s - loss: 0.0661 - val_loss: 0.1078\n",
      "Epoch 30/150\n",
      "508/508 [==============================] - 0s - loss: 0.0705 - val_loss: 0.1057\n",
      "Epoch 31/150\n",
      "508/508 [==============================] - 0s - loss: 0.0763 - val_loss: 0.1007\n",
      "Epoch 32/150\n",
      "508/508 [==============================] - 0s - loss: 0.0744 - val_loss: 0.1003\n",
      "Epoch 33/150\n",
      "508/508 [==============================] - 0s - loss: 0.0721 - val_loss: 0.1274\n",
      "Epoch 34/150\n",
      "508/508 [==============================] - 0s - loss: 0.0846 - val_loss: 0.1172\n",
      "Epoch 35/150\n",
      "508/508 [==============================] - 0s - loss: 0.0844 - val_loss: 0.1119\n",
      "Epoch 36/150\n",
      "508/508 [==============================] - 0s - loss: 0.0773 - val_loss: 0.0940\n",
      "Epoch 37/150\n",
      "508/508 [==============================] - 0s - loss: 0.0743 - val_loss: 0.1087\n",
      "Epoch 38/150\n",
      "508/508 [==============================] - 0s - loss: 0.0731 - val_loss: 0.1128\n",
      "Epoch 39/150\n",
      "508/508 [==============================] - 0s - loss: 0.0770 - val_loss: 0.1150\n",
      "Epoch 40/150\n",
      "508/508 [==============================] - 0s - loss: 0.0737 - val_loss: 0.1088\n",
      "Epoch 41/150\n",
      "508/508 [==============================] - 0s - loss: 0.0697 - val_loss: 0.1014\n",
      "Epoch 42/150\n",
      "508/508 [==============================] - 0s - loss: 0.0723 - val_loss: 0.0965\n",
      "Epoch 43/150\n",
      "508/508 [==============================] - 0s - loss: 0.0683 - val_loss: 0.1023\n",
      "Epoch 44/150\n",
      "508/508 [==============================] - 0s - loss: 0.0669 - val_loss: 0.1125\n",
      "Epoch 45/150\n",
      "508/508 [==============================] - 0s - loss: 0.0653 - val_loss: 0.1053\n",
      "Epoch 46/150\n",
      "508/508 [==============================] - 0s - loss: 0.0695 - val_loss: 0.1115\n",
      "Epoch 47/150\n",
      "508/508 [==============================] - 0s - loss: 0.0700 - val_loss: 0.1007\n",
      "Epoch 48/150\n",
      "508/508 [==============================] - 0s - loss: 0.0648 - val_loss: 0.1021\n",
      "Epoch 49/150\n",
      "508/508 [==============================] - 0s - loss: 0.0686 - val_loss: 0.1192\n",
      "Epoch 50/150\n",
      "508/508 [==============================] - 0s - loss: 0.0661 - val_loss: 0.1048\n",
      "Epoch 51/150\n",
      "508/508 [==============================] - 0s - loss: 0.0678 - val_loss: 0.1120\n",
      "Epoch 52/150\n",
      "508/508 [==============================] - 0s - loss: 0.0672 - val_loss: 0.1035\n",
      "Epoch 53/150\n",
      "508/508 [==============================] - 0s - loss: 0.0653 - val_loss: 0.1205\n",
      "Epoch 54/150\n",
      "508/508 [==============================] - 0s - loss: 0.0679 - val_loss: 0.1151\n",
      "Epoch 55/150\n",
      "508/508 [==============================] - 0s - loss: 0.0626 - val_loss: 0.1039\n",
      "Epoch 56/150\n",
      "508/508 [==============================] - 0s - loss: 0.0640 - val_loss: 0.1173\n",
      "Epoch 57/150\n",
      "508/508 [==============================] - 0s - loss: 0.0673 - val_loss: 0.1065\n",
      "Epoch 58/150\n",
      "508/508 [==============================] - 0s - loss: 0.0701 - val_loss: 0.1117\n",
      "Epoch 59/150\n",
      "508/508 [==============================] - 0s - loss: 0.0641 - val_loss: 0.1106\n",
      "Epoch 60/150\n",
      "508/508 [==============================] - 0s - loss: 0.0679 - val_loss: 0.1221\n",
      "Epoch 61/150\n",
      "508/508 [==============================] - 0s - loss: 0.0709 - val_loss: 0.1024\n",
      "Epoch 62/150\n",
      "508/508 [==============================] - 0s - loss: 0.0828 - val_loss: 0.1056\n",
      "Epoch 63/150\n",
      "508/508 [==============================] - 0s - loss: 0.0822 - val_loss: 0.1015\n",
      "Epoch 64/150\n",
      "508/508 [==============================] - 0s - loss: 0.0795 - val_loss: 0.1167\n",
      "Epoch 65/150\n",
      "508/508 [==============================] - 0s - loss: 0.0717 - val_loss: 0.1075\n",
      "Epoch 66/150\n",
      "508/508 [==============================] - 0s - loss: 0.0693 - val_loss: 0.1071\n",
      "Epoch 67/150\n",
      "508/508 [==============================] - 0s - loss: 0.0717 - val_loss: 0.1325\n",
      "Epoch 68/150\n",
      "508/508 [==============================] - 0s - loss: 0.0690 - val_loss: 0.1073\n",
      "Epoch 69/150\n",
      "508/508 [==============================] - 0s - loss: 0.0697 - val_loss: 0.0992\n",
      "Epoch 70/150\n",
      "508/508 [==============================] - 0s - loss: 0.0748 - val_loss: 0.1039\n",
      "Epoch 71/150\n",
      "508/508 [==============================] - 0s - loss: 0.0654 - val_loss: 0.1105\n",
      "Epoch 72/150\n",
      "508/508 [==============================] - 0s - loss: 0.0682 - val_loss: 0.1065\n",
      "Epoch 73/150\n",
      "508/508 [==============================] - 0s - loss: 0.0611 - val_loss: 0.1118\n",
      "Epoch 74/150\n",
      "508/508 [==============================] - 0s - loss: 0.0665 - val_loss: 0.0984\n",
      "Epoch 75/150\n",
      "508/508 [==============================] - 0s - loss: 0.0613 - val_loss: 0.1078\n",
      "Epoch 76/150\n",
      "508/508 [==============================] - 0s - loss: 0.0608 - val_loss: 0.1134\n",
      "Epoch 77/150\n",
      "508/508 [==============================] - 0s - loss: 0.0587 - val_loss: 0.1074\n",
      "Epoch 78/150\n",
      "508/508 [==============================] - 0s - loss: 0.0565 - val_loss: 0.1150\n",
      "Epoch 79/150\n",
      "508/508 [==============================] - 0s - loss: 0.0630 - val_loss: 0.1106\n",
      "Epoch 80/150\n",
      "508/508 [==============================] - 0s - loss: 0.0722 - val_loss: 0.1040\n",
      "Epoch 81/150\n",
      "508/508 [==============================] - 0s - loss: 0.0648 - val_loss: 0.1144\n",
      "Epoch 82/150\n",
      "508/508 [==============================] - 0s - loss: 0.0619 - val_loss: 0.1111\n",
      "Epoch 83/150\n",
      "508/508 [==============================] - 0s - loss: 0.0658 - val_loss: 0.1118\n",
      "Epoch 84/150\n",
      "508/508 [==============================] - 0s - loss: 0.0600 - val_loss: 0.1091\n",
      "Epoch 85/150\n",
      "508/508 [==============================] - 0s - loss: 0.0633 - val_loss: 0.1151\n",
      "Epoch 86/150\n",
      "508/508 [==============================] - 0s - loss: 0.0627 - val_loss: 0.1049\n",
      "Epoch 87/150\n",
      "508/508 [==============================] - 0s - loss: 0.0590 - val_loss: 0.1107\n",
      "Epoch 88/150\n",
      "508/508 [==============================] - 0s - loss: 0.0594 - val_loss: 0.1031\n",
      "Epoch 89/150\n",
      "508/508 [==============================] - 0s - loss: 0.0578 - val_loss: 0.1093\n",
      "Epoch 90/150\n",
      "508/508 [==============================] - 0s - loss: 0.0611 - val_loss: 0.1156\n",
      "Epoch 91/150\n",
      "508/508 [==============================] - 0s - loss: 0.0609 - val_loss: 0.0953\n",
      "Epoch 92/150\n",
      "508/508 [==============================] - 0s - loss: 0.0606 - val_loss: 0.1221\n",
      "Epoch 93/150\n",
      "508/508 [==============================] - 0s - loss: 0.0588 - val_loss: 0.1035\n",
      "Epoch 94/150\n",
      "508/508 [==============================] - 0s - loss: 0.0580 - val_loss: 0.1281\n",
      "Epoch 95/150\n",
      "508/508 [==============================] - 0s - loss: 0.0529 - val_loss: 0.1061\n",
      "Epoch 96/150\n",
      "508/508 [==============================] - 0s - loss: 0.0533 - val_loss: 0.1148\n",
      "Epoch 97/150\n",
      "508/508 [==============================] - 0s - loss: 0.0543 - val_loss: 0.1224\n",
      "Epoch 98/150\n",
      "508/508 [==============================] - 0s - loss: 0.0586 - val_loss: 0.1101\n",
      "Epoch 99/150\n",
      "508/508 [==============================] - 0s - loss: 0.0584 - val_loss: 0.1091\n",
      "Epoch 100/150\n",
      "508/508 [==============================] - 0s - loss: 0.0562 - val_loss: 0.1213\n",
      "Epoch 101/150\n",
      "508/508 [==============================] - 0s - loss: 0.0549 - val_loss: 0.1090\n",
      "Epoch 102/150\n",
      "508/508 [==============================] - 0s - loss: 0.0558 - val_loss: 0.1289\n",
      "Epoch 103/150\n",
      "508/508 [==============================] - 0s - loss: 0.0530 - val_loss: 0.1018\n",
      "Epoch 104/150\n",
      "508/508 [==============================] - 0s - loss: 0.0569 - val_loss: 0.1237\n",
      "Epoch 105/150\n",
      "508/508 [==============================] - 0s - loss: 0.0493 - val_loss: 0.1112\n",
      "Epoch 106/150\n",
      "508/508 [==============================] - 0s - loss: 0.0524 - val_loss: 0.1185\n",
      "Epoch 107/150\n",
      "508/508 [==============================] - 0s - loss: 0.0549 - val_loss: 0.1086\n",
      "Epoch 108/150\n",
      "508/508 [==============================] - 0s - loss: 0.0531 - val_loss: 0.1107\n",
      "Epoch 109/150\n",
      "508/508 [==============================] - 0s - loss: 0.0539 - val_loss: 0.1274\n",
      "Epoch 110/150\n",
      "508/508 [==============================] - 0s - loss: 0.0504 - val_loss: 0.1129\n",
      "Epoch 111/150\n",
      "508/508 [==============================] - 0s - loss: 0.0494 - val_loss: 0.1436\n",
      "Epoch 112/150\n",
      "508/508 [==============================] - 0s - loss: 0.0519 - val_loss: 0.1080\n",
      "Epoch 113/150\n",
      "508/508 [==============================] - 0s - loss: 0.0553 - val_loss: 0.1419\n",
      "Epoch 114/150\n",
      "508/508 [==============================] - 0s - loss: 0.0525 - val_loss: 0.1162\n",
      "Epoch 115/150\n",
      "508/508 [==============================] - 0s - loss: 0.0511 - val_loss: 0.1235\n",
      "Epoch 116/150\n",
      "508/508 [==============================] - 0s - loss: 0.0567 - val_loss: 0.1172\n",
      "Epoch 117/150\n",
      "508/508 [==============================] - 0s - loss: 0.0547 - val_loss: 0.1174\n",
      "Epoch 118/150\n",
      "508/508 [==============================] - 0s - loss: 0.0517 - val_loss: 0.1157\n",
      "Epoch 119/150\n",
      "508/508 [==============================] - 0s - loss: 0.0559 - val_loss: 0.1087\n",
      "Epoch 120/150\n",
      "508/508 [==============================] - 0s - loss: 0.0516 - val_loss: 0.1171\n",
      "Epoch 121/150\n",
      "508/508 [==============================] - 0s - loss: 0.0498 - val_loss: 0.1227\n",
      "Epoch 122/150\n",
      "508/508 [==============================] - 0s - loss: 0.0494 - val_loss: 0.1200\n",
      "Epoch 123/150\n",
      "508/508 [==============================] - 0s - loss: 0.0456 - val_loss: 0.1104\n",
      "Epoch 124/150\n",
      "508/508 [==============================] - 0s - loss: 0.0520 - val_loss: 0.1269\n",
      "Epoch 125/150\n",
      "508/508 [==============================] - 0s - loss: 0.0533 - val_loss: 0.1158\n",
      "Epoch 126/150\n",
      "508/508 [==============================] - 0s - loss: 0.0538 - val_loss: 0.1229\n",
      "Epoch 127/150\n",
      "508/508 [==============================] - 0s - loss: 0.0536 - val_loss: 0.1049\n",
      "Epoch 128/150\n",
      "508/508 [==============================] - 0s - loss: 0.0484 - val_loss: 0.1195\n",
      "Epoch 129/150\n",
      "508/508 [==============================] - 0s - loss: 0.0426 - val_loss: 0.1107\n",
      "Epoch 130/150\n",
      "508/508 [==============================] - 0s - loss: 0.0464 - val_loss: 0.1160\n",
      "Epoch 131/150\n",
      "508/508 [==============================] - 0s - loss: 0.0479 - val_loss: 0.1225\n",
      "Epoch 132/150\n",
      "508/508 [==============================] - 0s - loss: 0.0491 - val_loss: 0.1250\n",
      "Epoch 133/150\n",
      "508/508 [==============================] - 0s - loss: 0.0452 - val_loss: 0.1189\n",
      "Epoch 134/150\n",
      "508/508 [==============================] - 0s - loss: 0.0441 - val_loss: 0.1217\n",
      "Epoch 135/150\n",
      "508/508 [==============================] - 0s - loss: 0.0463 - val_loss: 0.1245\n",
      "Epoch 136/150\n",
      "508/508 [==============================] - 0s - loss: 0.0477 - val_loss: 0.1120\n",
      "Epoch 137/150\n",
      "508/508 [==============================] - 0s - loss: 0.0482 - val_loss: 0.1347\n",
      "Epoch 138/150\n",
      "508/508 [==============================] - 0s - loss: 0.0486 - val_loss: 0.1210\n",
      "Epoch 139/150\n",
      "508/508 [==============================] - 0s - loss: 0.0487 - val_loss: 0.1329\n",
      "Epoch 140/150\n",
      "508/508 [==============================] - 0s - loss: 0.0445 - val_loss: 0.1238\n",
      "Epoch 141/150\n",
      "508/508 [==============================] - 0s - loss: 0.0475 - val_loss: 0.1251\n",
      "Epoch 142/150\n",
      "508/508 [==============================] - 0s - loss: 0.0443 - val_loss: 0.1272\n",
      "Epoch 143/150\n",
      "508/508 [==============================] - 0s - loss: 0.0450 - val_loss: 0.1345\n",
      "Epoch 144/150\n",
      "508/508 [==============================] - 0s - loss: 0.0462 - val_loss: 0.1232\n",
      "Epoch 145/150\n",
      "508/508 [==============================] - 0s - loss: 0.0499 - val_loss: 0.1262\n",
      "Epoch 146/150\n",
      "508/508 [==============================] - 0s - loss: 0.0430 - val_loss: 0.1266\n",
      "Epoch 147/150\n",
      "508/508 [==============================] - 0s - loss: 0.0414 - val_loss: 0.1274\n",
      "Epoch 148/150\n",
      "508/508 [==============================] - 0s - loss: 0.0433 - val_loss: 0.1205\n",
      "Epoch 149/150\n",
      "508/508 [==============================] - 0s - loss: 0.0439 - val_loss: 0.1271\n",
      "Epoch 150/150\n",
      "508/508 [==============================] - 0s - loss: 0.0404 - val_loss: 0.1228\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size = 96,\n",
    "                    nb_epoch=150,\n",
    "                    validation_data=(X_valid, Y_valid),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014183\t0.003948\t\n",
      "0.000000\t-0.031058\t\n",
      "-0.300000\t0.009484\t\n",
      "-0.300000\t0.005337\t\n",
      "-0.000000\t-0.006013\t\n",
      "-0.009191\t-0.015654\t\n",
      "0.046632\t0.003594\t\n",
      "-0.000000\t-0.004185\t\n",
      "0.000000\t0.000471\t\n",
      "-0.119028\t-0.005344\t\n",
      "0.213890\t0.003605\t\n",
      "0.064105\t0.017826\t\n",
      "-0.031636\t-0.001545\t\n",
      "0.000000\t-0.005993\t\n",
      "-0.300000\t0.010110\t\n",
      "-0.014183\t0.000652\t\n",
      "-0.104052\t-0.132000\t\n",
      "-0.000000\t-0.029337\t\n",
      "0.000000\t-0.010760\t\n",
      "-0.126517\t-0.004734\t\n",
      "0.000000\t-0.024179\t\n",
      "-0.086550\t-0.032931\t\n",
      "-0.213890\t-0.006575\t\n",
      "-0.178935\t0.018968\t\n",
      "0.163958\t0.001379\t\n",
      "-0.000000\t0.003433\t\n",
      "-0.213890\t0.139114\t\n",
      "-0.029159\t-0.000960\t\n",
      "-0.046632\t-0.143306\t\n",
      "0.136501\t0.014278\t\n",
      "-0.000000\t0.001673\t\n",
      "-0.233858\t-0.051530\t\n",
      "0.000000\t0.001288\t\n",
      "-0.000000\t0.002375\t\n",
      "0.263811\t-0.036241\t\n",
      "0.099040\t0.021021\t\n",
      "0.031636\t-0.002330\t\n",
      "0.000000\t-0.002195\t\n",
      "-0.000000\t-0.025079\t\n",
      "-0.138997\t-0.000329\t\n",
      "0.021652\t-0.067185\t\n",
      "-0.300000\t0.001078\t\n",
      "0.216386\t-0.042051\t\n",
      "-0.300000\t-0.025090\t\n",
      "-0.129013\t0.001485\t\n",
      "-0.216386\t0.114091\t\n",
      "0.300000\t-0.043524\t\n",
      "0.300000\t-0.005425\t\n",
      "0.000000\t0.032213\t\n",
      "-0.049128\t0.000711\t\n",
      "0.173942\t0.012147\t\n",
      "0.000000\t-0.066183\t\n",
      "0.000000\t-0.020410\t\n",
      "0.009191\t-0.000033\t\n",
      "-0.000000\t0.002264\t\n",
      "-0.163958\t-0.031382\t\n",
      "0.178935\t0.000181\t\n",
      "0.031636\t0.018859\t\n",
      "-0.148982\t0.000710\t\n",
      "0.106548\t0.001007\t\n",
      "0.193911\t0.003777\t\n",
      "0.300000\t0.040643\t\n",
      "-0.116532\t0.002353\t\n",
      "0.300000\t-0.003264\t\n",
      "0.300000\t-0.007506\t\n",
      "0.166454\t-0.000483\t\n",
      "-0.099040\t-0.050179\t\n",
      "0.029159\t-0.000013\t\n",
      "-0.000000\t-0.042072\t\n",
      "0.096564\t0.003617\t\n",
      "0.034152\t-0.000102\t\n",
      "0.000000\t0.001551\t\n",
      "-0.000000\t0.009105\t\n",
      "-0.096564\t-0.080897\t\n",
      "-0.000000\t0.001675\t\n",
      "0.000000\t-0.005435\t\n",
      "-0.126517\t0.001686\t\n",
      "-0.300000\t0.000721\t\n",
      "-0.178935\t0.001132\t\n",
      "-0.044117\t-0.062014\t\n",
      "0.021671\t0.006240\t\n",
      "0.138997\t0.011954\t\n",
      "-0.213890\t0.245161\t\n",
      "0.129013\t-0.007236\t\n",
      "-0.041640\t0.002370\t\n",
      "0.300000\t-0.001909\t\n",
      "-0.193911\t-0.033676\t\n",
      "0.104052\t0.007172\t\n",
      "-0.106548\t0.024220\t\n",
      "-0.300000\t0.003318\t\n",
      "-0.000000\t-0.046953\t\n",
      "0.126517\t-0.014818\t\n",
      "-0.173942\t-0.005868\t\n",
      "-0.000000\t-0.009159\t\n",
      "0.213890\t0.001644\t\n",
      "0.000000\t0.001437\t\n",
      "-0.129013\t0.000782\t\n",
      "-0.049128\t0.001699\t\n",
      "0.300000\t0.017616\t\n",
      "-0.039144\t-0.002999\t\n",
      "0.000000\t0.001542\t\n",
      "-0.000000\t0.000023\t\n",
      "-0.300000\t0.001992\t\n",
      "-0.000000\t0.000336\t\n",
      "0.081577\t0.002234\t\n",
      "-0.000000\t-0.023925\t\n",
      "0.000000\t-0.026334\t\n",
      "0.178935\t0.001236\t\n",
      "-0.064105\t-0.004692\t\n",
      "0.046613\t0.002454\t\n",
      "0.119028\t0.023237\t\n",
      "0.000000\t-0.033680\t\n",
      "-0.000000\t-0.110664\t\n",
      "0.086550\t0.014881\t\n",
      "-0.136501\t0.002931\t\n",
      "0.116532\t0.009182\t\n",
      "0.000000\t-0.014921\t\n",
      "-0.300000\t-0.041837\t\n",
      "0.000000\t0.000279\t\n",
      "-0.000000\t-0.247407\t\n",
      "-0.000000\t0.002636\t\n",
      "0.300000\t0.037692\t\n",
      "-0.000000\t0.000431\t\n",
      "-0.263811\t-0.007676\t\n",
      "0.000000\t0.003165\t\n",
      "0.129013\t-0.015171\t\n",
      "0.151478\t0.001800\t\n",
      "0.000000\t0.001508\t\n",
      "-0.000000\t-0.000270\t\n",
      "-0.261315\t-0.096648\t\n",
      "-0.000000\t-0.001933\t\n",
      "0.049128\t0.001081\t\n",
      "0.049128\t0.002476\t\n",
      "0.064105\t-0.110595\t\n",
      "-0.034152\t0.062739\t\n",
      "0.213890\t0.009314\t\n",
      "0.039144\t0.000218\t\n",
      "-0.151478\t-0.000945\t\n",
      "-0.166454\t0.007039\t\n",
      "-0.000000\t-0.040836\t\n",
      "0.000000\t0.001275\t\n",
      "-0.001702\t0.001824\t\n",
      "0.001702\t-0.003080\t\n",
      "0.000000\t-0.038726\t\n",
      "-0.021671\t0.008220\t\n",
      "-0.084074\t-0.001760\t\n",
      "-0.000000\t0.001175\t\n",
      "0.000000\t0.001173\t\n",
      "-0.000000\t0.000523\t\n",
      "0.148982\t0.001290\t\n",
      "0.000000\t-0.015428\t\n",
      "-0.000000\t0.002196\t\n",
      "0.000000\t-0.008513\t\n",
      "-0.300000\t0.001719\t\n",
      "0.000000\t-0.032762\t\n",
      "-0.000000\t0.001745\t\n",
      "-0.000000\t0.000782\t\n",
      "0.000000\t0.001707\t\n",
      "0.300000\t-0.006869\t\n",
      "-0.081577\t0.001945\t\n",
      "0.000000\t-0.009536\t\n",
      "0.261315\t0.000324\t\n",
      "0.044117\t0.006650\t\n",
      "-0.000000\t-0.066091\t\n",
      "0.000000\t-0.002926\t\n",
      "0.126517\t-0.005516\t\n",
      "0.233858\t0.000318\t\n",
      "-0.064105\t0.002023\t\n",
      "0.084074\t0.009327\t\n",
      "-0.046613\t-0.002272\t\n",
      "0.300000\t-0.147244\t\n",
      "0.041640\t0.004193\t\n",
      "0.000000\t0.001385\t\n",
      "0.000000\t0.001317\t\n",
      "0.000000\t0.001487\t\n",
      "-0.000000\t-0.013134\t\n",
      "0.000000\t-0.026264\t\n",
      "0.000000\t0.001159\t\n",
      "0.000000\t-0.000414\t\n",
      "0.000000\t0.008885\t\n"
     ]
    }
   ],
   "source": [
    "conv_predictions = X_valid[:,-1]\n",
    "recurrent_predictions = model.predict(X_valid)\n",
    "ground_truth = Y_valid\n",
    "for x,y,z in zip(conv_predictions, ground_truth, recurrent_predictions):\n",
    "    print (\"{:07f}\\t{:07f}\\t\".format(y,z[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analyze training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.tsplot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:deepLearning]",
   "language": "python",
   "name": "conda-env-deepLearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
